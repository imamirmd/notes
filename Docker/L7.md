## Network Components 

### CNM (Container Network Model)

> **CNM** is Docker’s **networking specification** (the model). It defines **what networking components exist** and **how they relate**, not how they’re implemented.

- **Sandbox**
	- Represents the container’s **network namespace**
	- Holds interfaces, routes, DNS config
	- One sandbox ≈ one container network stack

- **Endpoint**
	- Connects a sandbox to a network
	- Represents one **network interface**
	- A container can have **multiple endpoints** (multi-network)

- **Network**
	- A logical network (bridge, overlay, etc.)
	- Defines connectivity and IPAM scope

### libnetwork

> **libnetwork** is Docker’s **implementation of CNM**.

> **CNM** is design, and **libnetwork** is engine that follows the design.

- Written in Go
- Runs inside the Docker Engine
- Responsible for:
    - Creating networks
    - Managing endpoints
    - Setting up namespaces
    - Calling network drivers
    - Managing IPAM

### Network Drivers

> Drivers are **pluggable backends** that implement how connectivity actually works.

> libnetwork talks to drivers using a **driver API**.

| **Common built-in drivers:**

- **bridge**
	The bridge driver is Docker’s default networking mode on a single host and connects containers using a Linux bridge and veth pairs. Each container runs in its own network sandbox and is attached to the bridge through an endpoint, allowing containers on the same bridge network to communicate with each other while using NAT for outbound traffic. This driver is best suited for local development and single-host deployments.

- **overlay**
	The overlay driver enables containers running on different Docker hosts to communicate as if they were on the same network by encapsulating traffic using VXLAN tunnels. It relies on Docker Swarm or a distributed key-value store and provides built-in service discovery across hosts. This driver is used for multi-host and distributed microservice architectures.

- **host**
	The host driver removes network isolation by letting the container share the host’s network namespace directly. In this mode, the container does not get its own sandbox or virtual interfaces and instead uses the host’s IP addresses and ports, which provides the highest performance but eliminates network isolation. It is commonly used for performance-critical or low-latency applications.

- **none**
	The none driver creates a network sandbox for the container but does not attach any endpoints to it, leaving the container with no external network connectivity. Only the loopback interface is available, making this driver ideal for highly secure, isolated workloads or batch jobs that do not require network access.

	The `none` network driver’s purpose is to guarantee **zero connectivity**, so libnetwork blocks attaching additional networks to that sandbox. Allowing `docker network connect` would break the contract of the `none` driver.

- **macvlan**
	The macvlan driver assigns each container its own MAC address and IP address on the physical network, making the container appear as a separate machine on the LAN. It bypasses Docker’s bridge and NAT mechanisms, which allows direct Layer 2 connectivity but requires proper network configuration and often prevents direct host-to-container communication. This driver is useful for legacy applications or network appliances.

**Note**: Two Docker containers can see and communicate with each other **only if their sandboxes are connected to the same network through endpoints**.

> In Docker networking, **CNM** defines the abstract model (network, endpoint, and sandbox), **libnetwork** is the Docker Engine component that implements this model and manages networking logic, and **network drivers** are pluggable backends that actually create connectivity using Linux primitives like bridges, VXLAN, or macvlan. libnetwork orchestrates everything by creating sandboxes, endpoints, and networks according to CNM, then delegates the real networking work to the selected driver.

> **libnetwork** is responsible for **creating and managing** the CNM objects (sandbox, endpoint, and network) and for **matching them together**, while the **network driver** is responsible for **actually implementing** those objects using the underlying OS networking features. libnetwork decides _what_ needs to exist and _how components are connected_, and then calls the driver to do the low-level work such as creating interfaces, bridges, VXLAN tunnels, or MAC addresses.

```shell
sudo docker network ls
```

```shell
sudo docker network ls -f driver=bridge
```

```shell
sudo docker network inspect <network>
```

```shell
sudo docker network inspect bridge -f "{{.Containers}}"
```

**Create a network**

```shell
sudo docker create <name> (--driver <driver = bridge>)
```

```shell
sudo docker run -it --network <name> busybox sh
```

**Change default IP & Gateway**

```shell
sudo docker create --subnet <subnet-id>/<mask> --gateway <gateway> <name>
```

**Using another container's network**

```shell
sudo docker run -it --network container:<name> busybox sh
```

**Static IP Address**

```shell
sudo docker run -it --network <name> --ip <ip> busybox sh
```

**Remove a network**

```shell
sudo docker network rm  <name>
```

---
### User-defined Network VS. Default Network 

> The **default bridge network** (`bridge`) is automatically created by Docker and provides basic connectivity between containers, but it has limited features: containers can only communicate reliably using IP addresses, **DNS-based name resolution is not fully supported**, and all containers attached to it share the same broad network space, which reduces isolation.

> A **user-defined network** (usually a user-defined bridge) is created explicitly by the user and offers much better behavior. Containers on a user-defined network can communicate using **container names via Docker’s built-in DNS**, are isolated from containers on other networks by default, and allow more control over IP addressing, subnets, and network policies. Because of this, user-defined networks are the recommended choice for most applications, especially in production environments.

---
### Docker `connect`

> Connects an **existing container** to an **existing Docker network** by creating a new **endpoint** for it.

```shell
sudo docker network connect <network> <container>
```

- `--ip <IP>`  
    Assign a specific IPv4 address

- `--ip6 <IPv6>`  
    Assign a specific IPv6 address

- `--alias <name>`  
    Add an extra DNS alias for the container on that network

- `--driver-opt <key=value>`  
    Pass driver-specific options

---
### Docker `disconnect`

> Disconnects a container from a Docker network by **removing the endpoint** between them.

```shell
sudo docker network disconnect <network> <container>
```

- `-f, --force`  
	Force disconnect even if the container is running

---
### Port Mapping

> Port mapping exposes a **container’s port** on the **host**, allowing external traffic to reach the container.

> Port mapping uses NAT to forward traffic from a host port to a container port, making the container accessible from outside Docker.
> 

```shell
sudo docker run -p <host_port>:<container_port> <image>
```

- Port mapping is **only needed with bridge/overlay**
- **Not needed** with `--network host`
- Multiple containers **cannot** bind the same host port
- Mapping is **host → container**, not container → container

```shell
sudo docker port <container>
```

**Forwarding should be enabled**

```shell
sysctl net.ipv4.conf.all.forwarding=1
```

```shell
sudo iptables -P FORWARD ACCEPT
```

---
### `macvlan` (Docker Network Driver)

> `macvlan` lets a container appear as a **real device on the physical network**, with its **own MAC and IP address**, just like a physical machine.

| **How it works**

- Docker attaches the container directly to a **physical network interface**
- Each container gets:    
    - Its own **MAC address**
    - Its own **IP address**
    
- Traffic bypasses Docker bridge and NAT

```shell
sudo docker network create -d macvlan \
  --subnet=<subnet-id>/<mask> \
  --gateway=<gateway> \
  -o parent=<NIC> \
  <name>
```

**VLAN Segmentation Scenario**

> A Layer 2 switch is connected to a Docker host. Two VLANs are configured on this switch. The connection between the switch and the Docker host operates in trunk mode, allowing multiple VLANs to pass through. The objective is to create two separate containers, each assigned to a specific VLAN.

- VLAN 10 -> 192.168.10.0/24
- VLAN 20 -> 192.168.20.0/24

```shell
# VLAN 10
sudo docker network create -d macvlan \
 --subnet=192.168.10.0/24 \
 --ip-range=192.168.10.0/25 \
 --gateway=192.168.10.1 \
 -o parent=eth0.10 \
 vlan10
```

```shell
# VLAN 20
sudo docker network create -d macvlan \
 --subnet=192.168.20.0/24 \
 --ip-range=192.168.20.0/25 \
 --gateway=192.168.20.1 \
 -o parent=eth0.20 \
 vlan20
```

---
### Service Discovery 

> Docker containers use an internal DNS server (`127.0.0.11`) for service discovery, which forwards unresolved queries to the host’s global DNS.

---





