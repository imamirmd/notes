### Basic Definitions

> **DevOps** is a way of working where developers and operations teams work closely together to build, test, and release software faster and more reliably.

> A **DevOps engineer** is a person who helps developers and operations teams work together by automating processes, managing systems, and making sure software is built, tested, and deployed smoothly and reliably.

> An **artifact** is a file or result produced during software development, such as a build file, package, or compiled code, that is stored and used later for testing or deployment.

> A **pipeline** is an automated sequence of steps that code goes through - such as build, test, and deploy - from the moment it is committed until it is released.

> **Staging** is a testing environment that is very similar to production, where code is checked one last time before being released to real users.

> **On-demand** means something happens **only when it is requested**, not automatically or continuously.

---
### CI/CD Concept

> **CI (Continuous Integration)** is a practice where developers regularly merge their code into a shared repository and automatically test it to catch problems early. (focuses on **testing code**)

- Code is merged and **automatically built and tested** to find bugs early. (_check if the code works_)

> **CD (Continuous Delivery / Continuous Deployment)** is a practice where tested code is automatically prepared or released to production, so new changes can reach users quickly and safely. (focuses on **releasing code**)

- After CI passes, code is **automatically delivered or deployed** to servers.  (_send the code to users_)

---
### Architectures 

> **Monolithic architecture** is a software design where the entire application is built and deployed as one single, tightly connected unit.

> **Microservice architecture** is a software design where an application is built as a collection of small, independent services that communicate with each other and can be developed, deployed, and scaled separately.

> **Layered architecture** is a software design where an application is organized into separate layers, and each layer has a specific responsibility (such as presentation, business logic, and data).

---
### Isolation 

> **Bare metal** refers to running software directly on physical hardware without a virtualization layer, meaning the operating system runs straight on the server itself.

- Software runs **directly on physical hardware**
- Very fast and powerful
- Harder to manage and scale

> **Virtualization** is a technology that allows multiple virtual machines to run on a single physical server by sharing its hardware resources.

- Runs **multiple virtual machines**, each with its own OS
- Better isolation than bare metal
- Uses more resources because each VM has a full OS

> **Containerization** is a method of running applications in lightweight, isolated containers that package the app with everything it needs to run consistently across different environments.

- Runs apps in **containers that share the same OS**
- Very lightweight and fast
- Easy to deploy, scale, and manage

| Isolation        | Benefit                   |
| ---------------- | ------------------------- |
| Bare metal       | direct and powerful       |
| Virtualization   | isolated but heavy        |
| Containerization | lightweight and efficient |

---
### Docker Concept

> **Docker** is a platform that lets you build, package, and run applications inside containers so they work the same way on any system.

> **Docker image** is a read-only template that contains an application and everything it needs to run, used to create Docker containers.

- The **application code**
- The **runtime** (for example, Python or Node.js)
- **Libraries and dependencies**
- **System tools** and required files
- **Configuration instructions**

> **Docker container** is a running instance of a Docker image that executes an application in an isolated environment on the host system.

- A **small writable layer** for runtime changes (To the upper layer of the image)

> **Docker host** is the machine (physical or virtual) that runs Docker and provides the resources needed to run Docker containers.

> **Docker registry** is a storage service where Docker images are saved, shared, and downloaded for use by Docker hosts.

- **Registry** → the server that stores images
- **Repositories** → collections of related images (usually one per app)
- **Images** → specific builds inside a repository
- **Tags** → labels for image versions (like `latest`, `v1.0`)
- **Layers** → reusable filesystem layers that make up an image

> **Docker manifest** is a metadata file that describes **one or more image variants** under a single image name.

- One image name (e.g. `nginx:latest`)
- Multiple images for different **architectures or OS** (amd64, arm64, etc.)
- Docker pulls the **correct image automatically** for your system

**Note**: Linux Docker images can run on Windows using a Linux VM (WSL2 or Hyper-V), but Windows images require the Windows kernel and cannot run on Linux.

**Note**: Containers run applications by sharing the host OS kernel, making them lightweight and fast, while **virtual machines** run full operating systems, making them heavier but more isolated.

---
### Docker on Bare Metal vs. Virtual Machine

> Running **Docker on bare metal** uses the host’s hardware directly, giving better performance and lower overhead, while running **Docker inside a virtual machine** adds an extra layer of isolation but slightly reduces performance.

| **On Bare Meta**

- Docker runs directly on the host OS, which talks straight to the hardware.
- Containers share the **host kernel**, so there is **no extra virtualization layer**.
- This gives **better performance**, **lower latency**, and **less resource overhead**.
- Management is simpler, but containers are **closer to the host**, so isolation is weaker than a VM.

| **On virtual Machine**

- Docker runs inside a guest OS, which itself runs on a hypervisor.
- There is **double isolation**: container → VM → host.
- This improves **security and tenant isolation**, which is useful in cloud and multi-tenant setups.
- Performance is slightly lower because hardware access goes through the VM layer, and resource usage is higher.

---
### What does Docker solve?

> Docker makes applications **portable, consistent, fast to deploy, and easy to scale**.

- **Environment inconsistency**
	code runs the same on every system (“works on my machine” problem).

- **Dependency conflicts** 
	each app has its own libraries and versions.

- **Slow deployment**
	apps can be started, stopped, and moved very quickly.

- **Scaling difficulty** 
	containers are easy to replicate and scale.

- **Resource waste** 
	containers are lightweight compared to virtual machines.

- **Complex setup** 
	applications are packaged once and run anywhere.

---
### Container History

> The idea of **containers** started in the early 2000s with Unix technologies like **chroot**, **FreeBSD Jails**, and later **Linux cgroups and namespaces**, which allowed process isolation on the same OS.

| **1979 – chroot (Unix)** 

- Unix introduced `chroot`, which allowed a process to run with a limited view of the filesystem. 

- This was the first step toward isolation, but it only isolated files, not CPU, memory, or processes.

| **2000 – FreeBSD Jails** 

- FreeBSD added **Jails**, which isolated filesystem, users, network, and processes.

- This was the first system that looked very similar to modern containers.

| **2006–2008 – Linux cgroups and namespaces** 

- **Namespaces** isolate processes, users, networks, mounts
- **cgroups** limit and control CPU, memory, and I/O

 *Together, these features made **true OS-level containers possible**.*

| **2008 – LXC (Linux Containers)**

- LXC combined cgroups and namespaces into a usable container system.
- It worked, but was **complex and hard to manage**.

| **2013 – Docker**  (*Docker did **not invent containers**, but it*)

- Simplified container usage
- Added **images, layers, registries**
- Made containers **portable and developer-friendly**  

| **After Docker**

- **containerd**, **CRI-O** → container runtimes
- **Kubernetes** → container orchestration  

> Containers became the standard for cloud and microservices.

> Containers are **OS-level isolation**, not virtualization. Docker succeeded because it made existing kernel features **easy, repeatable, and practical**.

> Containers evolved from simple filesystem isolation into a full process-isolation model, and Docker turned them into a global standard.

---
### LXC VS. Docker

> **LXC** and **Docker** both use Linux container technology, but they have **different goals**.

| **LXC (Linux Containers)**

- Focuses on running **full system containers** (like lightweight VMs).
- You manage networking, storage, and lifecycle **manually**.
- Feels closer to a traditional server or VM.
- More control, but more complexity.

| **Docker**

- Strong tooling: images, layers, registries, CI/CD integration.
- Easy to build, ship, and run apps.
- Opinionated and developer-friendly.
- Focuses on running **single applications** in containers.

> Full system containers mean containers that behave like a *complete operating system environment*, running multiple processes (init system, services, users) similar to a lightweight virtual machine.

> Use **LXC** when you want VM-like control.  
> Use **Docker** when you want simple, portable app deployment.

---
### Docker Engine

> **Docker Engine** is the core software that builds, runs, and manages Docker containers on a system.

| **Docker Engine Components**

- **Docker daemon (`dockerd`)**
	runs in the background and manages containers, images, networks, and volumes

- **Docker API**
	lets tools and clients communicate with the daemon

- **Docker CLI (`docker`)** 
	the command-line tool users interact with

> Older Docker versions worked as a **single, tightly coupled system** where the **Docker daemon directly handled everything**.

- `dockerd` directly managed **images, networking, storage, and containers**
- It used **LXC at first**, and later its own runtime (`libcontainer`)
- There was **no containerd separation** like today
- All container lifecycle logic lived inside **dockerd**, making it large and less modular

> **LXC (Linux Containers)** provides OS-level isolation by using Linux **namespaces** and **cgroups** to run processes in isolated environments that behave like lightweight virtual machines.

- **Namespaces** 
	isolate what a process can _see_ (like processes, network, filesystem).

- **cgroups (control groups)**
	limit and control how much _resources_ (CPU, memory, I/O) a process can use.

> **LXC provides APIs and tools** to manage containers, including a **C API**, a **command-line interface (`lxc` tools)**, and **language bindings** (like Python) to communicate with and control containers.

> **libcontainer** is Docker’s low-level library that directly talks to the Linux kernel to create and manage containers using **namespaces and cgroups**, without relying on LXC.

**In short:**  

- **Old Docker**
	one big daemon doing everything  

- **New Docker**
	split into smaller components (dockerd + containerd + runc)

---
### Docker Engine Architecture

1. **Docker CLI**

	- The command-line tool (`docker run`, `docker build`, etc.)
	- Sends user commands to Docker Engine
	- Does **not** run containers itself

2. **Docker API**

	- A REST API used by the CLI and other tools
	- Communication happens over:
	    - **Unix socket** (`/var/run/docker.sock`) on Linux
	    - **TCP** (optional)

3. **Docker Daemon (`dockerd`)**

	- The main brain of Docker
	- Responsibilities:
	    - Image management (pull, build, store)
	    - Network management
	    - Volume management
	    - High-level container lifecycle decisions
	- Delegates low-level container work to `containerd`
	
4. **Containerd**

	- A container runtime manager
	- Focuses only on **container lifecycle**, not Docker-specific features
	- Responsibilities:
	    - Create, start, stop, delete containers
	    - Manage container metadata
	    - Pull and unpack images
	- Talks to `dockerd` via API
	- Spawns a **shim** per container

5. **containerd-shim**
	
	- A small helper process created **per container**
	- Responsibilities:
	    - Becomes the **parent process** of the container
	    - Keeps the container alive if `containerd` restarts
	    - Handles STDIN / STDOUT / STDERR
	    - Reports exit status back to containerd
	- Critical for container reliability
	 
6. **runc**

	- Low-level container runtime (OCI compliant)
	- Does the actual work of:
	    - Creating **namespaces**
	    - Applying **cgroups**
	    - Setting up the container filesystem
	- Starts the container’s main process
	- Exits after container startup
	
7.  **Linux Kernel**
	
	- The real executor
	- Provides:
	    - Namespaces (PID, network, mount, user, etc.)
	    - cgroups (CPU, memory, I/O limits)
	    - Security features (seccomp, AppArmor, SELinux)

---
### Unix Socket

> **Unix socket** is a special file that allows processes on the same system to communicate with each other efficiently without using network ports.

> **Unix socket in Docker** is a local communication file (like `/var/run/docker.sock`) that allows Docker clients and tools to talk to the Docker daemon securely on the same machine.

> To manage Docker as a **non-root user**, add your user to the **docker group**, which allows access to the Docker Unix socket without using `sudo`.

```shell
sudo usermod -aG docker (<username: str> | $USER)
```

---
### Container Layer ID

> **layer ID** is a unique identifier (hash) that represents one specific filesystem layer in a Docker image.

- Each **layer** created from a Dockerfile instruction gets its own **layer ID**
- The ID is a **hash**, based on the layer’s content
- If two layers have the same content, they get the **same layer ID** and can be shared

| **Why it matters?**

- Docker uses layer IDs to **reuse and cache layers**
- It avoids downloading or rebuilding identical layers
- Multiple images can **share the same layer** safely

---
### Container lifetime

> **Container lifetime** is the period of time a container exists - from when it is created until it stops or is removed.

- **Created**: container is set up from an image
- **Running**: the main process is active
- **Stopped**: the process ends or is stopped
- **Removed**: the container is deleted

| **Key point**

- A container lives **as long as its main process runs**
- When that process stops, the container stops

> Container lifetime depends on the **lifecycle of the main process inside it**

---
### Communication with dockerd

> There are **three main ways** to communicate with **`dockerd`**:

- **Unix socket** (`/var/run/docker.sock`) 
	the default and most common method on Linux

- **TCP socket**
	remote access over the network (optionally secured with TLS)

- **Docker API (REST)** 
	used by the Docker CLI and other tools, over Unix or TCP sockets

---
### OOM

> **OOM (Out Of Memory)** is a Linux kernel condition where the system runs out of available memory.

- The kernel activates the **OOM Killer**
- It forcefully **kills a process** (often a container’s main process) to free memory

```shell
sudo docker inspect busybox -f '{{.State.OOMKilled}}'
```

**In short:**  
OOM = not enough memory → kernel kills processes to survive
